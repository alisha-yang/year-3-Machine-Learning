{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Bohan Yang\n",
    "###### Student ID : 814642\n",
    "###### Python version: 3\n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explanations of inputs for the functions:\n",
    "# file is the \"name.csv\" file name string\n",
    "# data is the usable format of csv file (output of function preprocess)\n",
    "# model is a list of four lists: class names, class counts, conditional attributes value counts for each class, \n",
    "#   and conditional attributes value probability for each class\n",
    "# instance is a line of data, without the last column, which is the class\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "def preprocess(file):\n",
    "    new_data = []\n",
    "    f = open(file,'r')\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        new_data.append(line.strip().split(\",\"))\n",
    "    f.close()\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train(data):\n",
    "    class_names = []\n",
    "    class_count = []\n",
    "    condi_list = []\n",
    "    condi_count_list = []\n",
    "    class_dict = defaultdict(int)\n",
    "    \n",
    "    total_count = len(data)\n",
    "    attr_count = len(data[0])-1\n",
    "    EPSILON = 1/total_count\n",
    "    \n",
    "    #first and second return list: class names and counts \n",
    "    for line in data:\n",
    "        if line[-1] != \"?\":\n",
    "            class_dict[line[-1]] += 1\n",
    "    for i in class_dict.keys():\n",
    "        class_names.append(i)\n",
    "    for i in class_dict.values():\n",
    "        class_count.append(i)\n",
    "    \n",
    "    #list of dicts of value counts, and list of dicts of value probs\n",
    "    for c in class_names:                \n",
    "        class_condi = []\n",
    "        condi_count = []\n",
    "        for a in range(attr_count):\n",
    "            \n",
    "            attr_dict = defaultdict(float)\n",
    "            attr_count_dict = defaultdict(int)\n",
    "            #smoothing using epsilon\n",
    "            for line in data:\n",
    "                attr_count_dict[line[a]] = 0\n",
    "                attr_dict[line[a]] = EPSILON\n",
    "            for line in data:\n",
    "                if (line[-1] == c) & (line[a] != \"?\"):\n",
    "                    attr_dict[line[a]] += 1/class_dict[c]\n",
    "                    attr_count_dict[line[a]] += 1\n",
    "            class_condi.append(attr_dict)\n",
    "            condi_count.append(attr_count_dict)\n",
    "            \n",
    "        condi_list.append(class_condi)\n",
    "        condi_count_list.append(condi_count)\n",
    "        \n",
    "    model = (class_names, class_count,condi_count_list, condi_list)\n",
    "    return model        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['unacc', 'acc', 'vgood', 'good'],\n",
       " [1210, 384, 65, 69],\n",
       " [[defaultdict(int, {'vhigh': 360, 'high': 324, 'med': 268, 'low': 258}),\n",
       "   defaultdict(int, {'vhigh': 360, 'high': 314, 'med': 268, 'low': 268}),\n",
       "   defaultdict(int, {'2': 326, '3': 300, '4': 292, '5more': 292}),\n",
       "   defaultdict(int, {'2': 576, '4': 312, 'more': 322}),\n",
       "   defaultdict(int, {'small': 450, 'med': 392, 'big': 368}),\n",
       "   defaultdict(int, {'low': 576, 'med': 357, 'high': 277})],\n",
       "  [defaultdict(int, {'vhigh': 72, 'high': 108, 'med': 115, 'low': 89}),\n",
       "   defaultdict(int, {'vhigh': 72, 'high': 105, 'med': 115, 'low': 92}),\n",
       "   defaultdict(int, {'2': 81, '3': 99, '4': 102, '5more': 102}),\n",
       "   defaultdict(int, {'2': 0, '4': 198, 'more': 186}),\n",
       "   defaultdict(int, {'small': 105, 'med': 135, 'big': 144}),\n",
       "   defaultdict(int, {'low': 0, 'med': 180, 'high': 204})],\n",
       "  [defaultdict(int, {'vhigh': 0, 'high': 0, 'med': 26, 'low': 39}),\n",
       "   defaultdict(int, {'vhigh': 0, 'high': 13, 'med': 26, 'low': 26}),\n",
       "   defaultdict(int, {'2': 10, '3': 15, '4': 20, '5more': 20}),\n",
       "   defaultdict(int, {'2': 0, '4': 30, 'more': 35}),\n",
       "   defaultdict(int, {'small': 0, 'med': 25, 'big': 40}),\n",
       "   defaultdict(int, {'low': 0, 'med': 0, 'high': 65})],\n",
       "  [defaultdict(int, {'vhigh': 0, 'high': 0, 'med': 23, 'low': 46}),\n",
       "   defaultdict(int, {'vhigh': 0, 'high': 0, 'med': 23, 'low': 46}),\n",
       "   defaultdict(int, {'2': 15, '3': 18, '4': 18, '5more': 18}),\n",
       "   defaultdict(int, {'2': 0, '4': 36, 'more': 33}),\n",
       "   defaultdict(int, {'small': 21, 'med': 24, 'big': 24}),\n",
       "   defaultdict(int, {'low': 0, 'med': 39, 'high': 30})]],\n",
       " [[defaultdict(float,\n",
       "               {'vhigh': 0.29809936486072774,\n",
       "                'high': 0.26834729874502616,\n",
       "                'med': 0.22206630700948946,\n",
       "                'low': 0.21380184419957207}),\n",
       "   defaultdict(float,\n",
       "               {'vhigh': 0.29809936486072774,\n",
       "                'high': 0.26008283593510906,\n",
       "                'med': 0.22206630700948946,\n",
       "                'low': 0.22206630700948946}),\n",
       "   defaultdict(float,\n",
       "               {'2': 0.2700001913070096,\n",
       "                '3': 0.24851258800122508,\n",
       "                '4': 0.24190101775329118,\n",
       "                '5more': 0.24190101775329118}),\n",
       "   defaultdict(float,\n",
       "               {'2': 0.47661176155493723,\n",
       "                '4': 0.25842994337312564,\n",
       "                'more': 0.26669440618304274}),\n",
       "   defaultdict(float,\n",
       "               {'small': 0.3724795301499817,\n",
       "                'med': 0.3245456458524625,\n",
       "                'big': 0.30471093510866143}),\n",
       "   defaultdict(float,\n",
       "               {'low': 0.47661176155493723,\n",
       "                'med': 0.2956200260177526,\n",
       "                'high': 0.2295043235384151})],\n",
       "  [defaultdict(float,\n",
       "               {'vhigh': 0.18807870370370355,\n",
       "                'high': 0.28182870370370355,\n",
       "                'med': 0.30005787037037035,\n",
       "                'low': 0.23234953703703673}),\n",
       "   defaultdict(float,\n",
       "               {'vhigh': 0.18807870370370355,\n",
       "                'high': 0.2740162037037035,\n",
       "                'med': 0.30005787037037035,\n",
       "                'low': 0.2401620370370367}),\n",
       "   defaultdict(float,\n",
       "               {'2': 0.21151620370370347,\n",
       "                '3': 0.2583912037037034,\n",
       "                '4': 0.26620370370370344,\n",
       "                '5more': 0.26620370370370344}),\n",
       "   defaultdict(float,\n",
       "               {'2': 0.0005787037037037037,\n",
       "                '4': 0.5162037037037048,\n",
       "                'more': 0.484953703703705}),\n",
       "   defaultdict(float,\n",
       "               {'small': 0.2740162037037035,\n",
       "                'med': 0.35214120370370405,\n",
       "                'big': 0.3755787037037042}),\n",
       "   defaultdict(float,\n",
       "               {'low': 0.0005787037037037037,\n",
       "                'med': 0.4693287037037049,\n",
       "                'high': 0.5318287037037046})],\n",
       "  [defaultdict(float,\n",
       "               {'vhigh': 0.0005787037037037037,\n",
       "                'high': 0.0005787037037037037,\n",
       "                'med': 0.40057870370370374,\n",
       "                'low': 0.6005787037037034}),\n",
       "   defaultdict(float,\n",
       "               {'vhigh': 0.0005787037037037037,\n",
       "                'high': 0.20057870370370373,\n",
       "                'med': 0.40057870370370374,\n",
       "                'low': 0.40057870370370374}),\n",
       "   defaultdict(float,\n",
       "               {'2': 0.15442485754985757,\n",
       "                '3': 0.2313479344729345,\n",
       "                '4': 0.30827101139601143,\n",
       "                '5more': 0.30827101139601143}),\n",
       "   defaultdict(float,\n",
       "               {'2': 0.0005787037037037037,\n",
       "                '4': 0.4621171652421653,\n",
       "                'more': 0.539040242165242}),\n",
       "   defaultdict(float,\n",
       "               {'small': 0.0005787037037037037,\n",
       "                'med': 0.38519408831908836,\n",
       "                'big': 0.6159633190883187}),\n",
       "   defaultdict(float,\n",
       "               {'low': 0.0005787037037037037,\n",
       "                'med': 0.0005787037037037037,\n",
       "                'high': 1.000578703703702})],\n",
       "  [defaultdict(float,\n",
       "               {'vhigh': 0.0005787037037037037,\n",
       "                'high': 0.0005787037037037037,\n",
       "                'med': 0.33391203703703715,\n",
       "                'low': 0.66724537037037}),\n",
       "   defaultdict(float,\n",
       "               {'vhigh': 0.0005787037037037037,\n",
       "                'high': 0.0005787037037037037,\n",
       "                'med': 0.33391203703703715,\n",
       "                'low': 0.66724537037037}),\n",
       "   defaultdict(float,\n",
       "               {'2': 0.21797000805152983,\n",
       "                '3': 0.2614482689210951,\n",
       "                '4': 0.2614482689210951,\n",
       "                '5more': 0.2614482689210951}),\n",
       "   defaultdict(float,\n",
       "               {'2': 0.0005787037037037037,\n",
       "                '4': 0.5223178341384864,\n",
       "                'more': 0.4788395732689213}),\n",
       "   defaultdict(float,\n",
       "               {'small': 0.3049265297906603,\n",
       "                'med': 0.34840479066022556,\n",
       "                'big': 0.34840479066022556}),\n",
       "   defaultdict(float,\n",
       "               {'low': 0.0005787037037037037,\n",
       "                'med': 0.5657960950080515,\n",
       "                'high': 0.43536131239935605})]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#an example for running function train\n",
    "train(preprocess(\"car.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function predicts the class for an instance, based on a trained model \n",
    "def predict(model, instance): \n",
    "    num_class = len(model[0])\n",
    "    num_inst = sum(model[1])\n",
    "    poster_list = []\n",
    "    final = 0\n",
    "    for i in range(num_class):\n",
    "        cond = 0\n",
    "        for j in range(len(instance)): \n",
    "            #use log to avoid underflow\n",
    "            cond += np.log2(model[3][i][j][instance[j]])\n",
    "        poster_list.append(np.log2(model[1][i]/num_inst) + cond)  \n",
    "\n",
    "    for i in range(len(poster_list)):\n",
    "        if poster_list[i] == max(poster_list):\n",
    "            final = i\n",
    "    return model[0][final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#an example for running function predict\n",
    "model = train(preprocess(\"mushroom.csv\"))\n",
    "instance = preprocess(\"mushroom.csv\")[30][:-1]\n",
    "predict(model, instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first way to evaluate a Naive Bayes classifier\n",
    "def accuracy(model, data):\n",
    "    acc = 0\n",
    "    for line in data:\n",
    "        if predict(model, line[:-1]) == line[-1]:\n",
    "            acc += 1\n",
    "    print(acc/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9634416543574594\n"
     ]
    }
   ],
   "source": [
    "#example for running function accuracy\n",
    "data = preprocess(\"mushroom.csv\")\n",
    "model = train(data)\n",
    "accuracy(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second way to evaluate a Naive Bayes classifier \n",
    "#This function prints out the TN, TP, FN, FP's of the Naive Bayes prediction. \n",
    "#The metrics can be calculated by hand easily with the output\n",
    "def evaluate(model, data):\n",
    "    TN = defaultdict(int)\n",
    "    TP = defaultdict(int)\n",
    "    FN = defaultdict(int)\n",
    "    FP = defaultdict(int)\n",
    "    \n",
    "    for c in model[0]:\n",
    "        for line in data:\n",
    "            if (predict(model, line[:-1]) == c) and (line[-1] == c):\n",
    "                TP[c] += 1\n",
    "            elif (predict(model, line[:-1]) != c) and (line[-1] != c):\n",
    "                TN[c] += 1\n",
    "            elif (predict(model, line[:-1]) == c) and (line[-1] != c):\n",
    "                FP[c] += 1\n",
    "            elif (predict(model, line[:-1]) != c) and (line[-1] == c):    \n",
    "                FN[c] += 1\n",
    "    \n",
    "    print(\"       TP   TN   FN   FP\")\n",
    "    for attr in TP.keys():\n",
    "        print(attr,\":\", TP[attr], TN[attr], FN[attr], FP[attr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TP   TN   FN   FP\n",
      "unacc : 1162 433 48 85\n",
      "acc : 289 1224 95 120\n",
      "vgood : 37 1661 28 2\n",
      "good : 21 1647 48 12\n"
     ]
    }
   ],
   "source": [
    "#an example for running function evaluate\n",
    "data = preprocess(\"car.csv\")\n",
    "model = train(data)\n",
    "\n",
    "evaluate(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain(model):\n",
    "    num_inst = sum(model[1])\n",
    "    class_num = len(model[0])\n",
    "    attr_num = len(model[2][0])\n",
    "\n",
    "    #calculate H(R)\n",
    "    hr = 0\n",
    "    for c in model[1]:\n",
    "        hr -= (c/num_inst) * np.log2(c/num_inst)\n",
    "    \n",
    "    #calculate p(value) and h(value) for each attribute\n",
    "    p_list = []\n",
    "    h_list = []\n",
    "    total_list = []\n",
    "    for i in range(attr_num):\n",
    "        p_value = defaultdict(int)\n",
    "        t_value = defaultdict(int)\n",
    "        \n",
    "        for c in range(class_num):\n",
    "            \n",
    "            #total counts for the value\n",
    "            for k in model[3][c][i].keys():      \n",
    "                if k not in t_value.keys():\n",
    "                    t_value[k]= model[2][c][i][k]\n",
    "                else:\n",
    "                    t_value[k] += model[2][c][i][k]\n",
    "            \n",
    "            #calculate p(value) for each attribute\n",
    "            for k in model[3][c][i].keys():      \n",
    "                if k not in p_value.keys():\n",
    "                    p_value[k]= model[2][c][i][k]/num_inst\n",
    "                else:\n",
    "                    p_value[k] += model[2][c][i][k]/num_inst\n",
    "        \n",
    "        p_list.append(p_value)\n",
    "        total_list.append(t_value)\n",
    "\n",
    "    #calculate h(value) for each attribute\n",
    "    for i in range(attr_num):\n",
    "        h_value = defaultdict(float)\n",
    "        for c in range(class_num):\n",
    "            for k in model[3][c][i].keys():\n",
    "                if (model[2][c][i][k]!= 0) and (total_list[i][k] != 0):\n",
    "                    if k not in h_value.keys():\n",
    "                        h_value[k] = -((model[2][c][i][k]/total_list[i][k]) * np.log2(model[2][c][i][k]/total_list[i][k]))\n",
    "                    else:\n",
    "                        h_value[k] -= (model[2][c][i][k]/total_list[i][k]) * np.log2(model[2][c][i][k]/total_list[i][k])       \n",
    "        h_list.append(h_value)\n",
    "        \n",
    "    #calculate mean information for each attribute\n",
    "    mean_info = []\n",
    "    for i in range(attr_num):\n",
    "        temp = 0\n",
    "        for k in (p_list[i]).keys():\n",
    "            temp += (p_list[i][k]) * (h_list[i][k])\n",
    "        mean_info.append(temp)\n",
    "        \n",
    "    #calculate information gain for each attribute\n",
    "    info_gain = []\n",
    "    for i in mean_info:\n",
    "        info_gain.append(hr - i)\n",
    "        \n",
    "    return(info_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03660746514280977,\n",
       " 0.015265380561918285,\n",
       " 0.014490701150154384,\n",
       " 0.08645063847884216,\n",
       " 0.08322845589007444,\n",
       " 0.013806029835453981,\n",
       " 0.0903522944652434,\n",
       " 0.09049078626122986,\n",
       " 0.058739302370822144,\n",
       " 0.12938741279822152,\n",
       " 0.151635200236383,\n",
       " 0.10012174391687256,\n",
       " 0.08493296456638766]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#an example for running the function info_gain\n",
    "info_gain(train(preprocess(\"hepatitis.csv\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0882166374169252\n",
      "0.9576837416481069\n",
      " \n",
      "0.0379949774347087\n",
      "0.5057705363204344\n",
      " \n",
      "0.11441568230991828\n",
      "0.8732638888888888\n",
      " \n",
      "0.1614732050742847\n",
      "0.9030092592592592\n"
     ]
    }
   ],
   "source": [
    "print(sum(info_gain(train(preprocess(\"anneal.csv\"))))/len(info_gain(train(preprocess(\"anneal.csv\")))))\n",
    "accuracy(train(preprocess(\"anneal.csv\")), preprocess(\"anneal.csv\"))\n",
    "print(\" \")\n",
    "print(sum(info_gain(train(preprocess(\"cmc.csv\"))))/len(info_gain(train(preprocess(\"cmc.csv\")))))\n",
    "accuracy(train(preprocess(\"cmc.csv\")), preprocess(\"cmc.csv\"))\n",
    "print(\" \")\n",
    "print(sum(info_gain(train(preprocess(\"car.csv\"))))/len(info_gain(train(preprocess(\"car.csv\")))))\n",
    "accuracy(train(preprocess(\"car.csv\")), preprocess(\"car.csv\"))\n",
    "print(\" \")\n",
    "print(sum(info_gain(train(preprocess(\"nursery.csv\"))))/len(info_gain(train(preprocess(\"nursery.csv\")))))\n",
    "accuracy(train(preprocess(\"nursery.csv\")), preprocess(\"nursery.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for question 1:\n",
    "Since the accuracy function and train function here do not delete the instances with missing values, the accuracies for predictions of the dataset with missing values are underestimated. Therefore, the datasets with missing values are not taken into consideration in this specific analyzation. As the comparison between average information gain and accuracy of the Naïve Bayes classifier shown above, it appears that in general, higher information gain leads to higher accuracy for the predictions. Since the information gain calculates the reduction of entropy after knowing the attribute value, a higher information gain implies a less random sample after knowing the attribute value. Naïve Bayes classifier predicts classes basing on the probability of a certain class with the attribute values known. Therefore if the sample is less random with the knowledge of the attribute value, it is more predictable, thus leads to a higher Naïve Bayes classifier accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Question 3: Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the class of an instance with 1-R decision stump\n",
    "def decision_stump(model, instance):\n",
    "    IG = info_gain(model)\n",
    "    class_num = len(model[0])\n",
    "    \n",
    "    #picking the attribute to apply the rule with\n",
    "    rule_attr = 0\n",
    "    for i in range(len(IG)):\n",
    "        if IG[i] == max(IG):\n",
    "            rule_attr = i\n",
    "            \n",
    "    #initializing\n",
    "    max_count_dict = defaultdict(int)\n",
    "    for c in range(class_num):\n",
    "        for k in model[2][c][i].keys():\n",
    "            max_count_dict[k] = 0\n",
    "        \n",
    "    #recording most frequent class for each attribute value    \n",
    "    for c in range(class_num):\n",
    "        for k in model[2][c][i].keys():\n",
    "            if model[2][c][i][k] > max_count_dict[k]:\n",
    "                max_count_dict[k] = model[2][c][i][k]\n",
    "\n",
    "    max_class_dict = defaultdict(int)\n",
    "    for c in range(class_num):\n",
    "        for k in model[2][c][i].keys():\n",
    "            if model[2][c][i][k] == max_count_dict[k]:\n",
    "                max_class_dict[k] = model[0][c]\n",
    "                \n",
    "    #predicting for this instance\n",
    "    value = instance[i]\n",
    "    return max_class_dict[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the accuracy of the decision_stump for certain model and data\n",
    "def accuracy_DS(model, data):\n",
    "    acc = 0\n",
    "    for line in data:\n",
    "        if decision_stump(model, line[:-1]) == line[-1]:\n",
    "            acc += 1\n",
    "    print(acc/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data1]\n",
      "for cmc.csv:\n",
      "0.4270196877121521\n",
      "0.5057705363204344\n",
      " \n",
      "for car.csv:\n",
      "0.7002314814814815\n",
      "0.8732638888888888\n",
      " \n",
      "for mushroom.csv:\n",
      "0.690300344657804\n",
      "0.9634416543574594\n",
      " \n",
      "for primary-tumor.csv:\n",
      "0.24778761061946902\n",
      "0.5988200589970502\n",
      " \n",
      "for hepatitis.csv:\n",
      "0.7935483870967742\n",
      "0.8516129032258064\n"
     ]
    }
   ],
   "source": [
    "#comparison of the accuracy for the 2 classifiers\n",
    "print(\"[data1]\")\n",
    "print(\"for cmc.csv:\")\n",
    "accuracy_DS(train(preprocess(\"cmc.csv\")), preprocess(\"cmc.csv\"))\n",
    "accuracy(train(preprocess(\"cmc.csv\")), preprocess(\"cmc.csv\"))\n",
    "print(\" \")\n",
    "print(\"for car.csv:\")\n",
    "accuracy_DS(train(preprocess(\"car.csv\")), preprocess(\"car.csv\"))\n",
    "accuracy(train(preprocess(\"car.csv\")), preprocess(\"car.csv\"))\n",
    "print(\" \")\n",
    "print(\"for mushroom.csv:\")\n",
    "accuracy_DS(train(preprocess(\"mushroom.csv\")), preprocess(\"mushroom.csv\"))\n",
    "accuracy(train(preprocess(\"mushroom.csv\")), preprocess(\"mushroom.csv\"))\n",
    "print(\" \")\n",
    "print(\"for primary-tumor.csv:\")\n",
    "accuracy_DS(train(preprocess(\"primary-tumor.csv\")), preprocess(\"primary-tumor.csv\"))\n",
    "accuracy(train(preprocess(\"primary-tumor.csv\")), preprocess(\"primary-tumor.csv\"))\n",
    "print(\" \")\n",
    "print(\"for hepatitis.csv:\")\n",
    "accuracy_DS(train(preprocess(\"hepatitis.csv\")), preprocess(\"hepatitis.csv\"))\n",
    "accuracy(train(preprocess(\"hepatitis.csv\")), preprocess(\"hepatitis.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for question 3: \n",
    "From the comparisons of the accuracy for decision stump and Naïve Bayes shown above[data1], it is shown that decision stump generally has a lower accuracy than Naïve Bayes. This could be because the decision stump only takes one attribute into consideration in the decision-making process, and is not sensitive to the interaction between different attributes. For the datasets that do not contain large number of classes, such as cmc. csv, car. csv and hepatitis. csv, the difference between two accuracies are not too significant. \n",
    "\n",
    "However, the decision stump accuracy for primary-tumor. csv is shockingly low. Primary-tumor has more classes than attributes and has a large amount of missing values. This quality has caused low accuracy for both classifiers, but decision stump is extremely low. It indicates that decision stump does not work well when there are too many classes or missing values.  \n",
    "\n",
    "An example of another unfortunate situation the decision stump could encounter is shown below[data2]. The most frequent classes for each attribute value are the same, resulting in every instance getting classified into the same class. This could be another causation of the low accuracy. \n",
    "\n",
    "Another problem which happened to not appear in the test cases, is when the critical attribute value in the testing instance is missing. Then the 1R will not be able to give any class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_stump_print(model):\n",
    "    IG = info_gain(model)\n",
    "    class_num = len(model[0])\n",
    "    \n",
    "    #picking the attribute to apply the rule with\n",
    "    rule_attr = 0\n",
    "    for i in range(len(IG)):\n",
    "        if IG[i] == max(IG):\n",
    "            rule_attr = i\n",
    "            \n",
    "    #initializing\n",
    "    max_count_dict = defaultdict(int)\n",
    "    for c in range(class_num):\n",
    "        for k in model[2][c][i].keys():\n",
    "            max_count_dict[k] = 0\n",
    "        \n",
    "    #recording most frequent class for each attribute value    \n",
    "    for c in range(class_num):\n",
    "        for k in model[2][c][i].keys():\n",
    "            if model[2][c][i][k] > max_count_dict[k]:\n",
    "                max_count_dict[k] = model[2][c][i][k]\n",
    "\n",
    "    max_class_dict = defaultdict(int)\n",
    "    for c in range(class_num):\n",
    "        for k in model[2][c][i].keys():\n",
    "            if model[2][c][i][k] == max_count_dict[k]:\n",
    "                max_class_dict[k] = model[0][c]\n",
    "                \n",
    "    print(max_class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data2\n",
      "defaultdict(<class 'int'>, {'Good': 'No-use', 'Not-good': 'No-use'})\n"
     ]
    }
   ],
   "source": [
    "print(\"data2\")\n",
    "decision_stump_print(train(preprocess(\"cmc.csv\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
